{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5b9dfd-e5a3-485b-898b-800a22a5c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "import yaml\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from variable_mapping import variable_mapping\n",
    "import datetime\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import dotenv_values\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "from providentia.auxiliar import CURRENT_PATH, join\n",
    "\n",
    "PROVIDENTIA_ROOT = '/home/avilanov/software/Providentia/'\n",
    "nonghost_root = '/home/avilanov/data/providentia/obs/nonghost/'\n",
    "CURRENT_PATH = os.getcwd()\n",
    "sys.path = [path for path in sys.path if '../dependencies/GHOST_standards/' not in path]            \n",
    "sys.path.insert(1, os.path.join(CURRENT_PATH, '../dependencies/GHOST_standards/1.5'))\n",
    "from GHOST_standards import standard_parameters, get_standard_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3182f7a5-fb1e-4846-9477-52f9af6073d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages_dict = yaml.safe_load(open(join(PROVIDENTIA_ROOT, 'settings', 'internal', 'actris', 'coverages.yaml')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9045d825-9ec6-4733-bd45-6d0b210c5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a3455-efea-4d1c-a2ff-04c90b1d5ce8",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf9a556-fcd2-4926-9533-c1992a4ce0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [  'od380aero']\n",
    "resolution = 'hourly'\n",
    "\n",
    "#variables = ['sconco3']\n",
    "#resolution = 'hourly'\n",
    "target_start_date = datetime.datetime(2010, 1, 1, 0)\n",
    "target_end_date = datetime.datetime(2018, 12, 31, 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d3f7c-9f41-44b0-a4a6-dbe5d6116d5a",
   "metadata": {},
   "source": [
    "# Get ACTRIS variable mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1139992f-b571-4bc7-90ed-f0332a6fe40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variable_mapping_file():\n",
    "    result = {\n",
    "        value['preferred_term'].replace('\"', ''): {'var': key[2], 'units': key[0]}\n",
    "        for key, value in variable_mapping.items()\n",
    "    }\n",
    "    \n",
    "    with open('variable_mapping.yaml', 'w') as file:\n",
    "        yaml.dump(result, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b750ad32-4d97-4dca-8f2a-ed58d63b4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_variable_mapping_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd12cc4-508c-41f8-b6c3-6b3ef5e56eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_mapping = yaml.safe_load(open(join(CURRENT_PATH, 'variable_mapping.yaml')))\n",
    "variable_mapping = {k: v for k, v in variable_mapping.items() if k.strip() and v}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f1930-b854-4ea1-b85d-82c8b334eb3c",
   "metadata": {},
   "source": [
    "# Get BSC-ACTRIS parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86d22c1-7dbb-4410-99ff-4407b3e7c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actris_variables_file():\n",
    "    with open('actris_variables.csv', mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for key in variable_mapping.keys():\n",
    "            writer.writerow([key, variable_mapping[key]['var']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f7ca2b-b623-46c6-be98-fe5070c716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_actris_variables_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c118ac6-82c3-471a-861e-3ce3363e3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ghost_variables_file():    \n",
    "    with open('ghost_variables.csv', mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for key in standard_parameters.keys():\n",
    "            writer.writerow([standard_parameters[key]['long_parameter_name'], standard_parameters[key]['bsc_parameter_name'], ', '.join( standard_parameters[key]['ebas_parameter_name'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5cc7adb-4ba6-4c31-95a5-f901ebfe0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_ghost_variables_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06a99ac2-4ddd-4670-b1a6-b298fba7d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = yaml.safe_load(open(join(PROVIDENTIA_ROOT, 'settings', 'internal', 'actris', 'ghost_actris_variables.yaml')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd098d0-b432-4e4d-8259-bf99ff678b61",
   "metadata": {},
   "source": [
    "# Get BSC-ACTRIS metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61b59fb2-f770-4ba5-aba9-b000256e8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_metadata = get_standard_metadata({'standard_units': ''})\n",
    "ghost_metadata = list(standard_metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b650d39-fa05-497b-b6eb-66d2b9142d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actris_metadata = ['Conventions', 'featureType', 'title', 'keywords', 'id', 'naming_authority',\n",
    "                   'project', 'acknowledgement', 'doi', 'license', 'citation', 'summary', 'source', \n",
    "                   'institution', 'processing_level', 'date_created', 'date_metadata_modified', \n",
    "                   'creator_name', 'creator_type', 'creator_email', 'creator_institution',\n",
    "                   'contributor_name', 'contributor_role', 'publisher_type', 'publisher_name', \n",
    "                   'publisher_institution', 'publisher_email', 'publisher_url', 'geospatial_bounds', \n",
    "                   'geospatial_bounds_crs', 'geospatial_lat_min', 'geospatial_lat_max',\n",
    "                   'geospatial_lon_min', 'geospatial_lon_max', 'geospatial_vertical_min', \n",
    "                   'geospatial_vertical_max', 'geospatial_vertical_positive', 'time_coverage_start',\n",
    "                   'time_coverage_end', 'time_coverage_duration', 'time_coverage_resolution',\n",
    "                   'timezone', 'ebas_data_definition', 'ebas_data_license', 'ebas_citation', \n",
    "                   'ebas_set_type_code', 'ebas_timezone', 'ebas_file_name', 'ebas_represents_doi',\n",
    "                   'ebas_contains_doi', 'ebas_file_creation', 'ebas_export_state', 'ebas_export_filter',\n",
    "                   'ebas_startdate', 'ebas_revision_date', 'ebas_data_level', 'ebas_period_code',\n",
    "                   'ebas_resolution_code', 'ebas_sample_duration', 'ebas_orig_time_res',\n",
    "                   'ebas_station_code', 'ebas_platform_code', 'ebas_station_name', \n",
    "                   'ebas_station_wdca_id', 'ebas_station_gaw_id', 'ebas_station_gaw_name',\n",
    "                   'ebas_station_land_use', 'ebas_station_setting', 'ebas_station_gaw_type', \n",
    "                   'ebas_station_wmo_region', 'ebas_station_latitude', 'ebas_station_longitude', \n",
    "                   'ebas_station_altitude', 'ebas_measurement_height', 'ebas_regime', 'ebas_component',\n",
    "                   'ebas_matrix', 'ebas_laboratory_code', 'ebas_instrument_type', 'ebas_instrument_name',\n",
    "                   'ebas_instrument_manufacturer', 'ebas_instrument_model', \n",
    "                   'ebas_instrument_serial_number', 'ebas_method_ref', 'ebas_standard_method',\n",
    "                   'ebas_inlet_type', 'ebas_inlet_description', 'ebas_humidity_temperaure_control',\n",
    "                   'ebas_absorption_cross_section', 'ebas_organization', 'ebas_framework_acronym',\n",
    "                   'ebas_framework_name', 'ebas_framework_description', 'ebas_framework_contact_name',\n",
    "                   'ebas_framework_contact_email', 'ebas_originator', 'ebas_submitter', \n",
    "                   'ebas_acknowledgement', 'Metadata_Conventions', 'geospatial_lat_units', \n",
    "                   'geospatial_lon_units', 'comment', 'standard_name_vocabulary', 'history', \n",
    "                   'creator_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bb43695-27c9-40ae-a283-a94a761fdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata_dict = {'station_reference': 'ebas_station_code',\n",
    "                     'WIGOS_station_identifier': '',\n",
    "                     'station_timezone': 'timezone',\n",
    "                     'latitude': 'ebas_station_latitude',\n",
    "                     'longitude': 'ebas_station_longitude',\n",
    "                     'altitude': 'ebas_station_altitude',\n",
    "                     'sampling_height': 'ebas_measurement_height',\n",
    "                     'measurement_altitude': 'ebas_station_altitude', # no diff with altitude?\n",
    "                     'ellipsoid': '',\n",
    "                     'horizontal_datum': '',\n",
    "                     'vertical_datum': '',\n",
    "                     'projection': '',\n",
    "                     'distance_to_building': '',\n",
    "                     'distance_to_kerb': '',\n",
    "                     'distance_to_junction': '',\n",
    "                     'distance_to_source': '',\n",
    "                     'street_width': '',\n",
    "                     'street_type': '',\n",
    "                     'daytime_traffic_speed': '',\n",
    "                     'daily_passing_vehicles': '',\n",
    "                     'data_level': 'ebas_data_level',\n",
    "                     'climatology': '',\n",
    "                     'station_name': 'ebas_station_name',\n",
    "                     'city': '', # get from title\n",
    "                     'country': '',\n",
    "                     'administrative_country_division_1': '',\n",
    "                     'administrative_country_division_2': '',\n",
    "                     'population': '',\n",
    "                     'representative_radius': '',\n",
    "                     'network': 'naming_authority',\n",
    "                     'associated_networks': 'naming_authority',\n",
    "                     'area_classification': '',\n",
    "                     'station_classification': '',\n",
    "                     'main_emission_source': '',\n",
    "                     'land_use': 'ebas_station_land_use',\n",
    "                     'terrain': '',\n",
    "                     'measurement_scale': '',\n",
    "                     'ESDAC_Iwahashi_landform_classification': '',\n",
    "                     'ESDAC_modal_Iwahashi_landform_classification_5km': '',\n",
    "                     'ESDAC_modal_Iwahashi_landform_classification_25km': '',\n",
    "                     'ESDAC_Meybeck_landform_classification': '',\n",
    "                     'ESDAC_modal_Meybeck_landform_classification_5km': '',\n",
    "                     'ESDAC_modal_Meybeck_landform_classification_25km': '',\n",
    "                     'GHSL_settlement_model_classification': '',\n",
    "                     'GHSL_modal_settlement_model_classification_5km': '',\n",
    "                     'GHSL_modal_settlement_model_classification_25km': '',\n",
    "                     'Joly-Peuch_classification_code': '',\n",
    "                     'Koppen-Geiger_classification': '',\n",
    "                     'Koppen-Geiger_modal_classification_5km': '',\n",
    "                     'Koppen-Geiger_modal_classification_25km': '',\n",
    "                     'MODIS_MCD12C1_v6_IGBP_land_use': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_IGBP_land_use_5km': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_IGBP_land_use_25km': '',\n",
    "                     'MODIS_MCD12C1_v6_UMD_land_use': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_UMD_land_use_5km': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_UMD_land_use_25km': '',\n",
    "                     'MODIS_MCD12C1_v6_LAI': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_LAI_5km': '',\n",
    "                     'MODIS_MCD12C1_v6_modal_LAI_25km': '',\n",
    "                     'WMO_region': 'ebas_station_wmo_region',\n",
    "                     'WWF_TEOW_terrestrial_ecoregion': '',\n",
    "                     'WWF_TEOW_biogeographical_realm': '',\n",
    "                     'WWF_TEOW_biome': '',\n",
    "                     'UMBC_anthrome_classification': '',\n",
    "                     'UMBC_modal_anthrome_classification_5km': '',\n",
    "                     'UMBC_modal_anthrome_classification_25km': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_BC_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_CO_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_NH3_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_NMVOC_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_NOx_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_OC_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_PM10_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_biogenic_PM2.5_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_fossilfuel_PM2.5_emissions': '',\n",
    "                     'EDGAR_v4.3.2_annual_average_SO2_emissions': '',\n",
    "                     'ASTER_v3_altitude': '',\n",
    "                     'ETOPO1_altitude': '',\n",
    "                     'ETOPO1_max_altitude_difference_5km': '',\n",
    "                     'GHSL_built_up_area_density': '',\n",
    "                     'GHSL_average_built_up_area_density_5km': '',\n",
    "                     'GHSL_average_built_up_area_density_25km': '',\n",
    "                     'GHSL_max_built_up_area_density_5km': '',\n",
    "                     'GHSL_max_built_up_area_density_25km': '',\n",
    "                     'GHSL_population_density': '',\n",
    "                     'GHSL_average_population_density_5km': '',\n",
    "                     'GHSL_average_population_density_25km': '',\n",
    "                     'GHSL_max_population_density_5km': '',\n",
    "                     'GHSL_max_population_density_25km': '',\n",
    "                     'GPW_population_density': '',\n",
    "                     'GPW_average_population_density_5km': '',\n",
    "                     'GPW_average_population_density_25km': '',\n",
    "                     'GPW_max_population_density_5km': '',\n",
    "                     'GPW_max_population_density_25km': '',\n",
    "                     'NOAA-DMSP-OLS_v4_nighttime_stable_lights': '',\n",
    "                     'NOAA-DMSP-OLS_v4_average_nighttime_stable_lights_5km': '',\n",
    "                     'NOAA-DMSP-OLS_v4_average_nighttime_stable_lights_25km': '',\n",
    "                     'NOAA-DMSP-OLS_v4_max_nighttime_stable_lights_5km': '',\n",
    "                     'NOAA-DMSP-OLS_v4_max_nighttime_stable_lights_25km': '',\n",
    "                     'OMI_level3_column_annual_average_NO2': '',\n",
    "                     'OMI_level3_column_cloud_screened_annual_average_NO2': '',\n",
    "                     'OMI_level3_tropospheric_column_annual_average_NO2': '',\n",
    "                     'OMI_level3_tropospheric_column_cloud_screened_annual_average_NO2': '',\n",
    "                     'GSFC_coastline_proximity': '',\n",
    "                     'primary_sampling_type': '',\n",
    "                     'primary_sampling_instrument_name': '',\n",
    "                     'primary_sampling_instrument_documented_flow_rate': '',\n",
    "                     'primary_sampling_instrument_reported_flow_rate': '',\n",
    "                     'primary_sampling_process_details': '',\n",
    "                     'primary_sampling_instrument_manual_name': '',\n",
    "                     'primary_sampling_further_details': '',\n",
    "                     'sample_preparation_types': '',\n",
    "                     'sample_preparation_techniques': '',\n",
    "                     'sample_preparation_process_details': '',\n",
    "                     'sample_preparation_further_details': '',\n",
    "                     'measurement_methodology': 'ebas_method_ref',\n",
    "                     'measuring_instrument_name': 'ebas_instrument_name',\n",
    "                     'measuring_instrument_sampling_type': 'ebas_instrument_type',\n",
    "                     'measuring_instrument_documented_flow_rate': '',\n",
    "                     'measuring_instrument_reported_flow_rate': '',\n",
    "                     'measuring_instrument_process_details': '',\n",
    "                     'measuring_instrument_manual_name': '',\n",
    "                     'measuring_instrument_further_details': '',\n",
    "                     'measuring_instrument_reported_units': '',\n",
    "                     'measuring_instrument_reported_lower_limit_of_detection': '',\n",
    "                     'measuring_instrument_documented_lower_limit_of_detection': '',\n",
    "                     'measuring_instrument_reported_upper_limit_of_detection': '',\n",
    "                     'measuring_instrument_documented_upper_limit_of_detection': '',\n",
    "                     'measuring_instrument_reported_uncertainty': '',\n",
    "                     'measuring_instrument_documented_uncertainty': '',\n",
    "                     'measuring_instrument_reported_accuracy': '',\n",
    "                     'measuring_instrument_documented_accuracy': '',\n",
    "                     'measuring_instrument_reported_precision': '',\n",
    "                     'measuring_instrument_documented_precision': '',\n",
    "                     'measuring_instrument_reported_zero_drift': '',\n",
    "                     'measuring_instrument_documented_zero_drift': '',\n",
    "                     'measuring_instrument_reported_span_drift': '',\n",
    "                     'measuring_instrument_documented_span_drift': '',\n",
    "                     'measuring_instrument_reported_zonal_drift': '',\n",
    "                     'measuring_instrument_documented_zonal_drift': '',\n",
    "                     'measuring_instrument_reported_measurement_resolution': '',\n",
    "                     'measuring_instrument_documented_measurement_resolution': '',\n",
    "                     'measuring_instrument_reported_absorption_cross_section': '',\n",
    "                     'measuring_instrument_documented_absorption_cross_section': '',\n",
    "                     'measuring_instrument_inlet_information': 'ebas_inlet_description',\n",
    "                     'measuring_instrument_calibration_scale': '',\n",
    "                     'network_provided_volume_standard_temperature': '',\n",
    "                     'network_provided_volume_standard_pressure': '',\n",
    "                     'retrieval_algorithm': '',\n",
    "                     'principal_investigator_name': 'creator_name',\n",
    "                     'principal_investigator_institution': 'creator_institution',\n",
    "                     'principal_investigator_email_address': 'creator_email',\n",
    "                     'contact_name': 'publisher_name', # TODO: in doubt\n",
    "                     'contact_institution': 'publisher_institution', # TODO: in doubt\n",
    "                     'contact_email_address': 'publisher_email', # TODO: in doubt\n",
    "                     'meta_update_stamp': 'date_metadata_modified',\n",
    "                     'data_download_stamp': '', # TODO: Do we put our download date here?\n",
    "                     'data_revision_stamp': '',\n",
    "                     'network_sampling_details': '',\n",
    "                     'network_uncertainty_details': '',\n",
    "                     'network_maintenance_details': '',\n",
    "                     'network_qa_details': '',\n",
    "                     'network_miscellaneous_details': '',\n",
    "                     'data_licence': 'license',\n",
    "                     'process_warnings': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7909442a-f3c0-47f4-b514-85840a0a9274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'station_reference': 'ebas_station_code',\n",
       " 'station_timezone': 'timezone',\n",
       " 'latitude': 'ebas_station_latitude',\n",
       " 'longitude': 'ebas_station_longitude',\n",
       " 'altitude': 'ebas_station_altitude',\n",
       " 'sampling_height': 'ebas_measurement_height',\n",
       " 'measurement_altitude': 'ebas_station_altitude',\n",
       " 'data_level': 'ebas_data_level',\n",
       " 'station_name': 'ebas_station_name',\n",
       " 'network': 'naming_authority',\n",
       " 'associated_networks': 'naming_authority',\n",
       " 'land_use': 'ebas_station_land_use',\n",
       " 'WMO_region': 'ebas_station_wmo_region',\n",
       " 'measurement_methodology': 'ebas_method_ref',\n",
       " 'measuring_instrument_name': 'ebas_instrument_name',\n",
       " 'measuring_instrument_sampling_type': 'ebas_instrument_type',\n",
       " 'measuring_instrument_inlet_information': 'ebas_inlet_description',\n",
       " 'principal_investigator_name': 'creator_name',\n",
       " 'principal_investigator_institution': 'creator_institution',\n",
       " 'principal_investigator_email_address': 'creator_email',\n",
       " 'contact_name': 'publisher_name',\n",
       " 'contact_institution': 'publisher_institution',\n",
       " 'contact_email_address': 'publisher_email',\n",
       " 'meta_update_stamp': 'date_metadata_modified',\n",
       " 'data_licence': 'license'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_dict = {k: v for k, v in all_metadata_dict.items() if v != ''}\n",
    "metadata_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faeaba-c27c-4a5c-a114-f0f35131c508",
   "metadata": {},
   "source": [
    "# Functions to format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3592c2f7-7032-448b-af96-54a1caac42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_var(var):\n",
    "    files_per_var = {}\n",
    "    base_url = \"https://prod-actris-md.nilu.no/metadata/content\"\n",
    "\n",
    "    if var not in files_per_var:\n",
    "        files_per_var[var] = {}\n",
    "\n",
    "    variable_files = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        # set up URL with pagination\n",
    "        url = f\"{base_url}/{parameters_dict[var]}/page/{page}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # check if the response is valid and contains data\n",
    "        if response.status_code != 200:\n",
    "            print(\n",
    "                f\"Error fetching page {page}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # check if there's content in the data\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # loop through each entry in the data and get OPeNDAP URL\n",
    "        for item in data:\n",
    "            doi = item.get(\"md_identification\", {}).get(\n",
    "                \"identifier\", {}).get(\"pid\")\n",
    "            opendap_urls = [protocol_dict['dataset_url'] for protocol_dict in item.get(\n",
    "                'md_distribution_information', []) if protocol_dict.get('protocol') == 'OPeNDAP']\n",
    "\n",
    "            # print DOI and OPeNDAP URL if both are present\n",
    "            if doi and opendap_urls:\n",
    "                variable_files.append(opendap_urls)\n",
    "\n",
    "        # go to the next page\n",
    "        page += 1\n",
    "\n",
    "    files_per_var[var]['files'] = list(\n",
    "        itertools.chain.from_iterable(variable_files))\n",
    "\n",
    "    return files_per_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b0037b-e3b3-4eef-85b0-2194ba923719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_path(var):\n",
    "\n",
    "    alpha_var = ''.join(x for x in var if x.isalpha())\n",
    "    if alpha_var in ['lsco', 'absco', 'lbsco', 'odaero']:\n",
    "        path = join(PROVIDENTIA_ROOT, 'settings', 'internal', 'actris', f'files/{alpha_var}/files.yaml')\n",
    "    else:\n",
    "        path = join(PROVIDENTIA_ROOT, 'settings', 'internal', 'actris', f'files/{var}/files.yaml')\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5522c38-5f19-4056-91e3-4ecb251364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporally_average_data(combined_ds, resolution, year, month, var):\n",
    "\n",
    "    # get valid dates frequency\n",
    "    if resolution == 'hourly':\n",
    "        frequency = 'h'\n",
    "    elif resolution == 'daily':\n",
    "        frequency = 'D'\n",
    "    elif resolution == 'monthly':\n",
    "        frequency = 'MS'\n",
    "\n",
    "    # get start and end of period to construct valid dates\n",
    "    time = combined_ds.time.values\n",
    "    start_date = datetime.datetime(year, month, 1)\n",
    "    first_day_next_month = datetime.datetime(year, month % 12 + 1, 1) if month != 12 else datetime.datetime(year + 1, 1, 1)\n",
    "    end_date = first_day_next_month - datetime.timedelta(days=1)\n",
    "    valid_dates = pd.date_range(start=start_date, end=end_date, freq=frequency).to_numpy(dtype='datetime64[ns]')\n",
    "    \n",
    "    # initialise averaged data\n",
    "    averaged_data = np.empty((len(combined_ds.station.values), len(valid_dates)))\n",
    "    \n",
    "    for station_i, station in enumerate(combined_ds.station.values):\n",
    "        # initialise averaged data\n",
    "        station_averaged_data = []\n",
    "    \n",
    "        # read data per station\n",
    "        data = combined_ds[var].isel(station=station_i).values\n",
    "\n",
    "        # get indices where values are nan\n",
    "        valid_idxs = ~np.isnan(data)\n",
    "\n",
    "        # go to next station if all values are nan\n",
    "        if np.sum(valid_idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        # filter nan values\n",
    "        valid_time = time[valid_idxs]\n",
    "        valid_data = data[valid_idxs]\n",
    "\n",
    "        # calculate weighted averages\n",
    "        if len(valid_data) != 0:\n",
    "            for date in valid_dates:\n",
    "            \n",
    "                # get differences between valid time and actual times in nanoseconds\n",
    "                time_diffs = (valid_time - date).astype('timedelta64[ns]').astype(float)\n",
    "            \n",
    "                # get positive differences and negative differences to differentiate \n",
    "                # between the actual times that are earlier than the valid date (negative), and those that are later (positive)\n",
    "                positive_diffs = time_diffs[time_diffs > 0]\n",
    "                negative_diffs = time_diffs[time_diffs < 0]\n",
    "                \n",
    "                # find the closest actual time after the valid time\n",
    "                closest_positive = None\n",
    "                if len(positive_diffs) > 0:\n",
    "                    closest_positive_idx = np.abs(positive_diffs).argmin()\n",
    "                    closest_positive = positive_diffs[closest_positive_idx]\n",
    "                    closest_positive_time = valid_time[time_diffs == positive_diffs[closest_positive_idx]][0]\n",
    "                    closest_positive_value = valid_data[time_diffs == positive_diffs[closest_positive_idx]][0]\n",
    "            \n",
    "                # find the closest actual time before the valid time\n",
    "                closest_negative = None\n",
    "                if len(negative_diffs) > 0:\n",
    "                    closest_negative_idx = np.abs(negative_diffs).argmin()\n",
    "                    closest_negative = negative_diffs[closest_negative_idx]\n",
    "                    closest_negative_time = valid_time[time_diffs == negative_diffs[closest_negative_idx]][-1]\n",
    "                    closest_negative_value = valid_data[time_diffs == negative_diffs[closest_negative_idx]][-1]\n",
    "            \n",
    "                # when the valid time only has a value in one direction, get closest value without calculating weights\n",
    "                if closest_positive is None:\n",
    "                    value = closest_negative_value\n",
    "                elif closest_negative is None:\n",
    "                    value = closest_positive_value\n",
    "                # in the rest of cases, calculate weights of 2 closest values and make average\n",
    "                else:\n",
    "                    # get 2 closest times and make positive to be able to compare differences\n",
    "                    closest_diffs = np.abs([closest_negative, closest_positive])\n",
    "            \n",
    "                    # we do the reverse, since we want the differences to have a heavier weight if these are smaller (nearer the actual time)\n",
    "                    weights = 1 / closest_diffs\n",
    "                    \n",
    "                    # finally we normalize them to have values between 0 and 1\n",
    "                    weights_normalized = weights / np.sum(weights)\n",
    "            \n",
    "                    # get average\n",
    "                    value = np.average([closest_negative_value, closest_positive_value], weights=weights_normalized)\n",
    "        \n",
    "                # save averaged data\n",
    "                station_averaged_data.append(value)\n",
    "        \n",
    "            averaged_data[station_i, :] = station_averaged_data\n",
    "        else:\n",
    "            averaged_data[station_i, :] = [np.nan]*len(valid_dates)\n",
    "    \n",
    "    # create new variable with averaged data\n",
    "    combined_averaged_ds = xr.DataArray(\n",
    "        data=averaged_data,\n",
    "        coords={'station': combined_ds.station.values, 'time': valid_dates}, \n",
    "        dims=['station', 'time'],\n",
    "        attrs={'units': combined_ds[var].attrs['ebas_unit']})\n",
    "    \n",
    "    # drop old variable and associated time\n",
    "    combined_ds = combined_ds.drop_vars(var)\n",
    "    combined_ds = combined_ds.drop_dims('time')\n",
    "    \n",
    "    # add new variable\n",
    "    combined_ds[var] = combined_averaged_ds\n",
    "    \n",
    "    return combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d40b53-5d36-44be-84a9-cb85f2a60cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wavelength_var(actris_parameter):\n",
    "    wavelength_var = False\n",
    "    if actris_parameter in ['aerosol particle light absorption coefficient',\n",
    "                            'aerosol particle light hemispheric backscatter coefficient',\n",
    "                            'aerosol particle light scattering coefficient',\n",
    "                            'aerosol particle equivalent black carbon mass concentration']:\n",
    "        wavelength_var = True\n",
    "    return wavelength_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cb80baf-ca96-402b-b071-1dc1cb299dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_info(files, var, path):\n",
    "    \n",
    "    files_info = {}\n",
    "    tqdm_iter = tqdm(files,bar_format= '{l_bar}{bar}|{n_fmt}/{total_fmt}',desc=f\"    Creating information file ({len(files)})\")\n",
    "    for file in tqdm_iter:\n",
    "        # open file\n",
    "        try:\n",
    "            ds = xr.open_dataset(file)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # get resolution\n",
    "        coverage = ds.time_coverage_resolution\n",
    "        try:             \n",
    "            file_resolution = coverages_dict[coverage]\n",
    "        except:\n",
    "            file_resolution = f'Unrecognised ({coverage})'\n",
    "            \n",
    "        file_start_date = ds.time_coverage_start\n",
    "        file_end_date = ds.time_coverage_end\n",
    "        file_variables = list(ds.data_vars.keys())\n",
    "        files_info[file] = {}\n",
    "        files_info[file]['resolution'] = file_resolution\n",
    "        files_info[file]['start_date'] = file_start_date\n",
    "        files_info[file]['end_date'] = file_end_date\n",
    "        files_info[file]['variables'] = file_variables\n",
    "\n",
    "    # create file\n",
    "    datasets = {\n",
    "        url: data\n",
    "        for url, data in files_info.items()\n",
    "    }\n",
    "    if len(datasets) != 0:\n",
    "        path_dir = os.path.dirname(path)\n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "        with open(path, 'w') as file:\n",
    "            yaml.dump(datasets, file, default_flow_style=False)\n",
    "    else:\n",
    "        print(f'    Error: No data could be found for {var}')\n",
    "        \n",
    "    return files_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "373d8658-64a9-454c-8eda-1c7014ada283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(files, var, actris_parameter, resolution):\n",
    "    \n",
    "    # combine datasets that have the same variable and resolution\n",
    "    combined_ds_list = []\n",
    "    metadata = {}\n",
    "    metadata[resolution] = {}\n",
    "    \n",
    "    # get EBAS component\n",
    "    ebas_component = variable_mapping[actris_parameter]['var']\n",
    "\n",
    "    # initialise wavelength\n",
    "    wavelength = None\n",
    "\n",
    "    errors = {}\n",
    "    warnings = {}\n",
    "    tqdm_iter = tqdm(files,bar_format= '{l_bar}{bar}|{n_fmt}/{total_fmt}',desc=f\"    Reading data ({len(files)})\")\n",
    "    for i, file in enumerate(tqdm_iter):\n",
    "        \n",
    "        # open file\n",
    "        try:\n",
    "            ds = xr.open_dataset(file)\n",
    "        except:\n",
    "            errors[file] = 'Error opening file'\n",
    "            continue\n",
    "\n",
    "        # remove time duplicates if any (keep first)\n",
    "        ds = ds.sel(time=~ds['time'].to_index().duplicated())\n",
    "            \n",
    "        # assign station code as dimension\n",
    "        ds = ds.expand_dims(dim={'station': [i]})\n",
    "\n",
    "        # select data for that variable only\n",
    "        unformatted_units = variable_mapping[actris_parameter]['units']\n",
    "        units = unformatted_units.replace('/', '_per_').replace(' ', '_')\n",
    "        units_var = f'{ebas_component}_{units}'\n",
    "        possible_vars = [ebas_component, \n",
    "                         f'{ebas_component}_amean', \n",
    "                         units_var, \n",
    "                         f'{units_var}_amean']\n",
    "        if var in ['sconcso4', 'precso4']:\n",
    "            possible_vars.append(f'sulphate_corrected_{units}')\n",
    "        ds_var_exists = False\n",
    "        for possible_var in possible_vars:\n",
    "            if possible_var in ds:\n",
    "                ds_var = ds[possible_var]\n",
    "                ds_var_exists = True\n",
    "                break\n",
    "\n",
    "        # continue to next file if variable cannot be read\n",
    "        if not ds_var_exists:\n",
    "            errors[file] = f'No variable name matches for {possible_vars}. Existing keys: {list(ds.data_vars)}'\n",
    "            continue\n",
    "\n",
    "        # get lowest level if tower height is in coordinates\n",
    "        if 'Tower_inlet_height' in list(ds_var.coords):\n",
    "            warnings[file] = f'Taking data from first height (Tower_inlet_height={min(ds_var.Tower_inlet_height.values)})'\n",
    "            ds_var = ds_var.sel(Tower_inlet_height=min(ds_var.Tower_inlet_height.values), drop=True)\n",
    "\n",
    "        # get data at desired wavelength if wavelength is in coordinates\n",
    "        if 'Wavelength' in list(ds_var.coords) or 'Wavelengthx' in list(ds_var.coords):\n",
    "            # TODO: Review wavelength to choose for black carbon\n",
    "            if var == 'sconcbc':\n",
    "                wavelength = 370\n",
    "            # Get wavelength from variable name for other variables\n",
    "            else:\n",
    "                wavelength = float(re.findall(r'\\d+', var)[0])\n",
    "\n",
    "            # Select data for wavelength\n",
    "            found_wavelength = False\n",
    "            if 'Wavelengthx' in list(ds_var.coords):\n",
    "                if wavelength in ds_var.Wavelengthx.values:\n",
    "                    ds_var = ds_var.sel(Wavelengthx=wavelength, drop=True)\n",
    "                    found_wavelength = True\n",
    "                else:\n",
    "                    existing_wavelengths = f'Existing wavelengths: {ds_var.Wavelengthx.values}'\n",
    "            elif 'Wavelength' in list(ds_var.coords):\n",
    "                if wavelength in ds_var.Wavelength.values:\n",
    "                    ds_var = ds_var.sel(Wavelength=wavelength, drop=True)\n",
    "                    found_wavelength = True\n",
    "                else:\n",
    "                    existing_wavelengths = f'Existing wavelengths: {ds_var.Wavelength.values}'\n",
    "                    \n",
    "            if not found_wavelength:\n",
    "                warnings[file] = f'Data at {wavelength}nm could not be found. Existing wavelengths: {existing_wavelengths}'\n",
    "                continue             \n",
    "                \n",
    "        # remove artifact and fraction (sconcoc)\n",
    "        if 'Artifact' in list(ds_var.coords):\n",
    "            warnings[file] = f'Taking data from first artifact dimension (Artifact={ds_var.Artifact.values[0]})'\n",
    "            ds_var = ds_var.isel(Artifact=0, drop=True)\n",
    "        if 'Fraction' in list(ds_var.coords):\n",
    "            warnings[file] = f'Taking data from first fraction dimension (Fraction={ds_var.Fraction.values[0]})'\n",
    "            ds_var = ds_var.isel(Fraction=0, drop=True)\n",
    "\n",
    "        # avoid datasets that do not have defined units\n",
    "        if 'ebas_unit' not in ds_var.attrs:\n",
    "            errors[file] = f'No units were defined'\n",
    "            continue\n",
    "\n",
    "        # avoid datasets that do not have the same units as in variable mapping\n",
    "        if ds_var.attrs['ebas_unit'] != variable_mapping[actris_parameter]['units']:\n",
    "            errors[file] = f\"Units {ds_var.attrs['ebas_unit']} do not match those in variable mapping \"\n",
    "            errors[file] += f\"dictionary ({variable_mapping[actris_parameter]['units']})\"\n",
    "            continue\n",
    "                \n",
    "        # save metadata\n",
    "        for ghost_key, ebas_key in metadata_dict.items():\n",
    "            # create key if it does not exist\n",
    "            if ghost_key not in metadata[resolution].keys():\n",
    "                metadata[resolution][ghost_key] = []\n",
    "\n",
    "            # search value in var attrs\n",
    "            if ebas_key in ds_var.attrs.keys():\n",
    "                metadata[resolution][ghost_key].append(ds_var.attrs[ebas_key])\n",
    "            # search value in ds attrs\n",
    "            elif ebas_key in ds.attrs.keys():\n",
    "                metadata[resolution][ghost_key].append(ds.attrs[ebas_key])\n",
    "            # not found -> nan\n",
    "            else:\n",
    "                metadata[resolution][ghost_key].append(np.nan)\n",
    "\n",
    "        # remove all attributes except units\n",
    "        ds_var.attrs = {key: value for key, value in ds_var.attrs.items() if key == 'ebas_unit'}\n",
    "\n",
    "        # rename variable to BSC standards\n",
    "        ds_var = ds_var.to_dataset(name=var)\n",
    "\n",
    "        # append modified dataset to list\n",
    "        combined_ds_list.append(ds_var)\n",
    "    \n",
    "    # show errors\n",
    "    if len(errors) > 0:\n",
    "        print(f'\\nCollected errors ({len(errors)}):')\n",
    "        for file, error in errors.items():\n",
    "            print(f'{file} - Error: {error}')\n",
    "            \n",
    "    # show warnings\n",
    "    if len(warnings) > 0:\n",
    "        print(f'\\nCollected warnings ({len(warnings)}:')\n",
    "        for file, warning in warnings.items():\n",
    "            print(f'{file} - Warning: {warning}')\n",
    "            \n",
    "    return combined_ds_list, metadata, wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e5d3715-b9ba-4252-a18b-ee8869eb4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_download(nonghost_root, target_start_date, target_end_date, resolution, var):\n",
    "\n",
    "    base_dir = join(nonghost_root, 'actris/actris', resolution, var)\n",
    "    paths = []\n",
    "    current_date = copy.deepcopy(target_start_date)\n",
    "    while current_date <= target_end_date:\n",
    "        \n",
    "        # save path\n",
    "        path = f\"{base_dir}/{var}_{current_date.strftime('%Y%m')}.nc\"\n",
    "        paths.append(path)\n",
    "\n",
    "        # get following month\n",
    "        next_month = current_date.month % 12 + 1\n",
    "        next_year = current_date.year + (current_date.month // 12)\n",
    "        current_date = current_date.replace(year=next_year, month=next_month)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77903183-5a12-4ecb-9be9-d547b9b89bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_files_to_download(prov_start_time, nc_files_to_download):\n",
    "    \"\"\" Returns the files that are not already downloaded. \"\"\"\n",
    "    # initialise list of non-downloaded files\n",
    "    not_downloaded_files = []\n",
    "    \n",
    "    # get ssh user and password \n",
    "    env = dotenv_values(join(PROVIDENTIA_ROOT, \".env\"))\n",
    "    overwrite_choice = env.get(\"OVERWRITE\")\n",
    "\n",
    "    if nc_files_to_download:\n",
    "        # get the downloaded and not downloaded files\n",
    "        not_downloaded_files = list(filter(lambda x:not os.path.exists(x), nc_files_to_download))\n",
    "        downloaded_files = list(filter(lambda x:os.path.exists(x), nc_files_to_download))\n",
    "        \n",
    "        # get the files that were downloaded before the execution\n",
    "        downloaded_before_execution_files = list(filter(lambda x:prov_start_time > os.path.getctime(x), downloaded_files))\n",
    "\n",
    "        # if there was any file downloaded before the execution    \n",
    "        if downloaded_before_execution_files:\n",
    "            # make the user choose between overwriting or not overwriting\n",
    "            if overwrite_choice not in ['y','n']:\n",
    "                # ask if user wants to overwrite\n",
    "                while overwrite_choice not in ['y','n']:\n",
    "                    overwrite_choice = input(\"\\nThere are some files that were already downloaded in a previous download, do you want to overwrite them (y/n)? \").lower() \n",
    "                # ask if user wants to remember the decision\n",
    "                remind_txt = None\n",
    "                while remind_txt not in ['y','n']:\n",
    "                    remind_txt = input(\"\\nDo you want to remember your decision for future downloads (y/n)? \").lower() \n",
    "                # save the decision\n",
    "                if remind_txt == 'y':\n",
    "                    with open(join(PROVIDENTIA_ROOT, \".env\"),\"a\") as f:\n",
    "                        f.write(f\"OVERWRITE={overwrite_choice}\\n\")\n",
    "            # if user wants to overwrite then add the the files that were \n",
    "            # downloaded before the execution as if they were never download\n",
    "            if overwrite_choice == 'y':\n",
    "                not_downloaded_files += downloaded_before_execution_files\n",
    "            # change overwritten files boolean to True to indicate that some files were ignored\n",
    "            else:\n",
    "                overwritten_files_flag = True\n",
    "\n",
    "    return not_downloaded_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef46b1-9006-4071-b102-6f96f593e7eb",
   "metadata": {},
   "source": [
    "# Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "758bea82-74da-41da-a4cd-13455cb67930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Filtering files by resolution and dates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Reading data (78): 100%|████████████████████████████████████████████████|78/78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected warnings (73:\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/2K/T5/Y5/2KT5-Y553.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/39/YT/SJ/39YT-SJ8G.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 412.1 501.  862.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/4E/E2/8E/4EE2-8ERQ.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.3 412.4 501.4 862.2]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/65/2F/Z8/652F-Z8X6.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.8 411.9 500.4 862.9]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/6A/GQ/3Z/6AGQ-3ZV7.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 411.9 500.6 862.5]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/79/3J/AU/793J-AUUX.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.  411.9 499.7 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/7G/2J/PZ/7G2J-PZUF.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 501.  862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/7P/4G/C9/7P4G-C9N6.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 500. 610. 778. 812. 868.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/8E/J2/N5/8EJ2-N5C5.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 500. 812. 868.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/8G/MD/5W/8GMD-5WFC.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.  411.9 499.7 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/8S/KX/QS/8SKX-QSG3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.5 411.9 499.6 862.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/8Z/9Q/MX/8Z9Q-MX3Q.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 411.6 499.5 862.3]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/9P/GM/VJ/9PGM-VJZR.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/AP/JJ/JG/APJJ-JG8C.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 500.5 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/AU/86/FN/AU86-FNN6.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/AX/V8/WG/AXV8-WGJ6.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 411.6 499.5 862.3]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/AY/8T/3Z/AY8T-3ZZH.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/BD/WV/RR/BDWV-RRG3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [412. 500. 675. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/BG/XC/QU/BGXC-QUH5.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.8 411.8 500.4 861.8]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/CG/X4/V2/CGX4-V2AW.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 412.  501.  862.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/CW/48/2J/CW48-2JAE.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/CZ/25/K9/CZ25-K9D5.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.8 411.8 500.9 861.7]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/DG/SU/82/DGSU-82QS.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/ES/ZE/KB/ESZE-KBW3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 500. 812. 868.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/EU/UC/A8/EUUC-A8XC.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/EZ/MV/PC/EZMV-PC88.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.8 500.5 861.9]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/FT/WY/TX/FTWY-TXFC.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 500.9 863. ]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/FY/FR/MD/FYFR-MDAQ.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 412.1 501.  862.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/GA/ME/K3/GAME-K34Q.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/GD/XW/UW/GDXW-UWQ6.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.5 411.8 500.9 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/GK/8X/7S/GK8X-7SP3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 500.5 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/H8/NA/AH/H8NA-AHMS.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.8 412.  501.  862.2]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/HK/2Z/R9/HK2Z-R95J.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/HP/GJ/7J/HPGJ-7J9D.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.2 411.8 499.7 861.8]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/HQ/NX/3F/HQNX-3FNS.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/J5/JD/HE/J5JD-HEHJ.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/K8/PF/28/K8PF-28SD.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.8 411.9 500.4 862.9]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/KT/QM/6Q/KTQM-6QXD.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.1 411.3 500.4 860.8]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/MT/QD/9F/MTQD-9FSW.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [500. 610. 778. 812.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/MZ/24/XJ/MZ24-XJ8C.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/NJ/WE/C6/NJWE-C6ZW.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/PN/FS/KT/PNFS-KT66.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 501.  862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/PP/W8/F2/PPW8-F2YE.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.  412.  499.7 861.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/PU/QQ/3E/PUQQ-3E43.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/Q3/CS/UE/Q3CS-UEZA.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/Q6/VF/Z7/Q6VF-Z7PA.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/QM/ZW/XX/QMZW-XX3P.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 501.  863. ]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/R4/2G/2Y/R42G-2YZ3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 412.  862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/R8/ST/X9/R8ST-X93Z.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 412.  500.7 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/RE/DU/8K/REDU-8K2H.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.5 411.8 500.9 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/SB/JP/RX/SBJP-RX5M.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/SQ/X4/RD/SQX4-RDNA.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 412.1 501.  862.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/TA/TF/JS/TATF-JSJV.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.9 501.  863. ]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/TZ/7J/G8/TZ7J-G8BR.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [412. 500. 610. 778. 812.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/U3/EY/K3/U3EY-K3SH.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.2 411.8 499.7 861.8]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/U3/MD/VU/U3MD-VUVP.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 412.1 501.3 862.4]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/U3/XD/QW/U3XD-QW38.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.8 500.5 861.9]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/UJ/MW/U5/UJMW-U5E3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.  411.9 499.7 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/UP/6K/YE/UP6K-YEBX.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 812.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/VP/ZC/JJ/VPZC-JJQG.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 412.1 501.  863.1]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/VQ/44/T6/VQ44-T6GS.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/W7/MZ/R4/W7MZ-R4B3.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.6 411.4 500.5 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/WJ/UZ/BF/WJUZ-BFNB.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.9 412.1 499.7 862.2]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/X6/T9/Z7/X6T9-Z7BH.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.2 412.6 500.7 862.7]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/X9/62/H2/X962-H2CJ.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.9 412.  500.4 861.6]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/XX/6T/4Y/XX6T-4YEC.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 610. 778. 812. 868.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/YQ/S9/QW/YQS9-QWWK.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/YT/9T/EB/YT9T-EBRT.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [367.7 411.9 501.2 861.2]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/YZ/NZ/Z9/YZNZ-Z9FF.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368.5 411.8 499.5 862.5]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/Z5/9W/57/Z59W-57TG.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/ZE/G2/9S/ZEG2-9STU.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/ZF/RR/49/ZFRR-49YS.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [862.]\n",
      "https://thredds.nilu.no/thredds/dodsC/ebas_doi/ZZ/MJ/UK/ZZMJ-UKAR.nc - Warning: Data at 380.0nm could not be found. Existing wavelengths: Existing wavelengths: [368. 412. 500. 610. 778. 812.]\n",
      "    Combining files...\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201604.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201605.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201606.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201607.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201608.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201609.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201610.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201611.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201612.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201701.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201703.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201704.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201705.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201706.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201707.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201708.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201709.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201710.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201711.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201712.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201801.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201802.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201803.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201804.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201805.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201806.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201807.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201808.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201809.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201810.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201811.nc\n",
      "    Saved: /home/avilanov/data/providentia/obs/nonghost/actris/actris/hourly/od380aero/od380aero_201812.nc\n",
      "    Total number of saved files: 32\n"
     ]
    }
   ],
   "source": [
    "# get providentia start time\n",
    "prov_start_time = time.time()\n",
    "\n",
    "env = dotenv_values(join(PROVIDENTIA_ROOT, \".env\"))\n",
    "origin_update_choice = env.get(\"ORIGIN_UPDATE\")\n",
    "\n",
    "for var in variables:\n",
    "\n",
    "    # check if variable name is available\n",
    "    if var not in parameters_dict.keys():\n",
    "        print(f'Data for {var} cannot be downloaded')\n",
    "        continue\n",
    "    else:\n",
    "        actris_parameter = parameters_dict[var]\n",
    "    \n",
    "    # get files that were already downloaded\n",
    "    initial_check_nc_files = get_files_to_download(nonghost_root, target_start_date, target_end_date, resolution, var)\n",
    "    files_to_download = select_files_to_download(prov_start_time, initial_check_nc_files)\n",
    "    if not files_to_download:\n",
    "        msg = f\"\\nFiles were already downloaded for {var} at {resolution} \"\n",
    "        msg += f\"resolution between {target_start_date} and {target_end_date}.\"\n",
    "        print(msg)  \n",
    "        continue \n",
    "\n",
    "    # get files info path\n",
    "    path = get_files_path(var)\n",
    "    \n",
    "    # if file does not exist\n",
    "    if not os.path.isfile(path):\n",
    "        # get files information\n",
    "        print(f'\\nFile containing information of the files available in Thredds for {var} ({path}) does not exist, creating.')\n",
    "        combined_data = get_files_per_var(var)\n",
    "        all_files = combined_data[var]['files']\n",
    "        files_info = get_files_info(all_files, var, path)\n",
    "            \n",
    "    # if file exists\n",
    "    else:\n",
    "        # ask if user wants to update file information from NILU Thredds\n",
    "        if origin_update_choice not in ['y','n']:\n",
    "            while origin_update_choice not in ['y','n']:\n",
    "                origin_update_choice = input(f\"\\nFile containing information of the files available in Thredds for {var} ({path}) already exists. Do you want to update it (y/n)? \").lower() \n",
    "            # ask if user wants to remember the decision\n",
    "            remind_txt = None\n",
    "            while remind_txt not in ['y','n']:\n",
    "                remind_txt = input(\"\\nDo you want to remember your decision for future downloads (y/n)? \").lower() \n",
    "            # save the decision\n",
    "            if remind_txt == 'y':\n",
    "                with open(join(PROVIDENTIA_ROOT, \".env\"),\"a\") as f:\n",
    "                    f.write(f\"ORIGIN_UPDATE={origin_update_choice}\\n\")\n",
    "        if origin_update_choice == 'n':\n",
    "            # get files information\n",
    "            files_info = yaml.safe_load(open(join(CURRENT_PATH, path)))\n",
    "            files_info = {k: v for k, v in files_info.items() if k.strip() and v}\n",
    "        else:\n",
    "            # get files information\n",
    "            combined_data = get_files_per_var(var)\n",
    "            all_files = combined_data[var]['files']\n",
    "            files_info = get_files_info(all_files, var, path)\n",
    "    \n",
    "    # go to next variable if no data is found\n",
    "    if len(files_info) == 0:\n",
    "        continue\n",
    "        \n",
    "    # filter files by resolution and dates\n",
    "    print('    Filtering files by resolution and dates...')\n",
    "    files = []\n",
    "    for file, attributes in files_info.items():\n",
    "        if attributes[\"resolution\"] == resolution:\n",
    "            start_date = datetime.datetime.strptime(attributes[\"start_date\"], \"%Y-%m-%dT%H:%M:%S UTC\")\n",
    "            end_date = datetime.datetime.strptime(attributes[\"end_date\"], \"%Y-%m-%dT%H:%M:%S UTC\")\n",
    "            for file_to_download in files_to_download:\n",
    "                file_to_download_yearmonth = file_to_download.split(f'{var}_')[1].split('.nc')[0]\n",
    "                file_to_download_start_date = datetime.datetime.strptime(file_to_download_yearmonth, \"%Y%m\")\n",
    "                file_to_download_end_date = datetime.datetime(file_to_download_start_date.year, file_to_download_start_date.month, 1) + relativedelta(months=1, seconds=-1)\n",
    "                if file_to_download_start_date <= end_date and file_to_download_end_date >= start_date:\n",
    "                    if file not in files:\n",
    "                        files.append(file)\n",
    "    \n",
    "    if len(files) != 0:\n",
    "            \n",
    "        # get data and metadata for each file within period\n",
    "        combined_ds_list, metadata, wavelength = get_data(files, var, actris_parameter, resolution)\n",
    "    \n",
    "        # combine and create new dataset\n",
    "        print('    Combining files...')\n",
    "        try:\n",
    "            combined_ds = xr.concat(combined_ds_list, \n",
    "                                    dim='station', \n",
    "                                    combine_attrs='drop_conflicts')\n",
    "        except Exception as error:\n",
    "            print(f'Error: Datasets could not be combined - {error}')\n",
    "            continue\n",
    "        \n",
    "        # add metadata\n",
    "        for key, value in metadata[resolution].items():\n",
    "            if key in ['latitude', 'longitude']:\n",
    "                value = [float(val) for val in value]\n",
    "            elif key in ['altitude', 'measurement_altitude', 'sampling_height']:\n",
    "                value = [float(val.replace('m', '').strip()) if isinstance(val, str) else val for val in value]\n",
    "            combined_ds[key] = xr.Variable(data=value, dims=('station'))\n",
    "    \n",
    "        # add units for lat and lon\n",
    "        # TODO: Check attrs geospatial_lat_units and geospatial_lon_units\n",
    "        combined_ds.latitude.attrs['units'] = 'degrees_north'\n",
    "        combined_ds.longitude.attrs['units'] = 'degrees_east'\n",
    "    \n",
    "        # add general attrs\n",
    "        combined_ds.attrs['data_license'] = 'BSD-3-Clause. Copyright 2025 Alba Vilanova Cortezón'\n",
    "        combined_ds.attrs['source'] = 'Observations'\n",
    "        combined_ds.attrs['institution'] = 'Barcelona Supercomputing Center'\n",
    "        combined_ds.attrs['creator_name'] = 'Alba Vilanova Cortezón'\n",
    "        combined_ds.attrs['creator_email'] = 'alba.vilanova@bsc.es'\n",
    "        combined_ds.attrs['application_area'] = 'Monitoring atmospheric composition'\n",
    "        combined_ds.attrs['domain'] = 'Atmosphere'\n",
    "        combined_ds.attrs['observed_layer'] = 'Land surface'\n",
    "                \n",
    "        # save data per year and month\n",
    "        path = join(nonghost_root, f'actris/actris/{resolution}/{var}')\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        saved_files = 0\n",
    "        for year, ds_year in combined_ds.groupby('time.year'):\n",
    "            for month, ds_month in ds_year.groupby('time.month'):\n",
    "                filename = f\"{path}/{var}_{year}{month:02d}.nc\"\n",
    "                if filename in files_to_download:\n",
    "                    combined_ds_yearmonth = combined_ds.sel(time=f\"{year}-{month:02d}\")\n",
    "                    combined_ds_yearmonth = temporally_average_data(combined_ds_yearmonth, resolution, year, month, var)\n",
    "    \n",
    "                    # add title to attrs\n",
    "                    extra_info = ''\n",
    "                    wavelength_var = is_wavelength_var(actris_parameter)\n",
    "                    if wavelength_var and wavelength is not None:\n",
    "                        extra_info = f' at {wavelength}nm'\n",
    "                    combined_ds_yearmonth.attrs['title'] = f'Surface {parameters_dict[var]}{extra_info} in the ACTRIS network in {year}-{month:02d}.'\n",
    "    \n",
    "                    # order attrs\n",
    "                    custom_order = ['title', 'institution', 'creator_name', 'creator_email',\n",
    "                                    'source', 'application_area', 'domain', 'observed_layer',\n",
    "                                    'data_license']\n",
    "                    ordered_attrs = {key: combined_ds_yearmonth.attrs[key] \n",
    "                                    for key in custom_order \n",
    "                                    if key in combined_ds_yearmonth.attrs}\n",
    "                    combined_ds_yearmonth.attrs = ordered_attrs\n",
    "    \n",
    "                    # remove stations if all variable data is nan\n",
    "                    # previous_n_stations = len(combined_ds_yearmonth.station)\n",
    "                    combined_ds_yearmonth = combined_ds_yearmonth.dropna(dim=\"station\", subset=[var], how=\"all\")\n",
    "                    combined_ds_yearmonth = combined_ds_yearmonth.assign_coords(station=range(len(combined_ds_yearmonth.station)))\n",
    "                    # current_n_stations = len(combined_ds_yearmonth.station)\n",
    "                    # n_stations_diff = previous_n_stations - current_n_stations\n",
    "                    # if n_stations_diff > 0:\n",
    "                    #     print(f'    Data for {n_stations_diff} stations was removed because all data was NaN during {month}-{year}.')\n",
    "                    \n",
    "                    # save file\n",
    "                    combined_ds_yearmonth.to_netcdf(filename)\n",
    "\n",
    "                    # change permissions\n",
    "                    os.system(\"chmod 777 {}\".format(filename))\n",
    "                    print(f\"    Saved: {filename}\")\n",
    "                    saved_files += 1\n",
    "                    \n",
    "        print(f'    Total number of saved files: {saved_files}')\n",
    "\n",
    "    else:\n",
    "        print('    No files were found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdebd974-97e4-4346-b915-3ba6c73ae779",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff2cd513-f0d2-4ba2-8c3a-0b341814c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:09:23.782694\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - start_time\n",
    "print(f'Elapsed time: {time.strftime(\"%H:%M:%S.{}\".format(str(total_time % 1)[2:])[:15], time.gmtime(total_time))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702e54a2-e5ad-422a-bd72-0ccb383c557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 48kB\n",
      "Dimensions:                                 (station: 5, time: 721)\n",
      "Coordinates:\n",
      "  * time                                    (time) datetime64[ns] 6kB 2018-01...\n",
      "  * station                                 (station) int64 40B 0 1 2 3 4\n",
      "Data variables: (12/26)\n",
      "    station_reference                       (station) <U7 140B ...\n",
      "    station_timezone                        (station) <U3 60B ...\n",
      "    latitude                                (station) float64 40B ...\n",
      "    longitude                               (station) float64 40B ...\n",
      "    altitude                                (station) float64 40B ...\n",
      "    sampling_height                         (station) float64 40B ...\n",
      "    ...                                      ...\n",
      "    contact_name                            (station) <U56 1kB ...\n",
      "    contact_institution                     (station) <U56 1kB ...\n",
      "    contact_email_address                   (station) <U12 240B ...\n",
      "    meta_update_stamp                       (station) <U23 460B ...\n",
      "    data_licence                            (station) <U44 880B ...\n",
      "    od380aero                               (station, time) float64 29kB ...\n",
      "Attributes:\n",
      "    title:             Surface aerosol particle optical depth in the ACTRIS n...\n",
      "    institution:       Barcelona Supercomputing Center\n",
      "    creator_name:      Alba Vilanova Cortezón\n",
      "    creator_email:     alba.vilanova@bsc.es\n",
      "    source:            Observations\n",
      "    application_area:  Monitoring atmospheric composition\n",
      "    domain:            Atmosphere\n",
      "    observed_layer:    Land surface\n",
      "    data_license:      BSD-3-Clause. Copyright 2025 Alba Vilanova Cortezón\n"
     ]
    }
   ],
   "source": [
    "path = f'/home/avilanov/data/providentia/obs/nonghost/actris/actris/{resolution}/{var}/{var}_201801.nc'\n",
    "try:\n",
    "    test_data = xr.open_dataset(path)\n",
    "    print(test_data)\n",
    "except: \n",
    "    print(f'File {path} was not created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
